{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ijuptilk20/Ebola-Virus-Disease-Model/blob/main/Stock_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'sentiments-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F404535%2F775750%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240809%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240809T075957Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3897e46a82b5a8eaf9ea8455e9b4e98cd7df15e53c6780a64ed12364dd7b6e256e176b15f876465495b1d1f894e154c0170c5427c92d2a46b085cce22ac86ef4ea166f03677443cfe048d1aa6c1acf43d8015aa19c1cac04932cdeb5c1b12bc9bf8410f9ad50fc85c8ffee2da10283401730ef2d1bd8a843cce89480f4ce974ebc1eb8024f9373ea56fde7813f84d387a9ddf1b9f95a45f890076090ecf2e68f926280c1aa988fbd6923fc32d09805cd4633964850e5c37b93cac0ead1d6ff46e084414bd12bd65699963f8c95df37abf35c2f4482fa917c6525fed4c455ff167525368abdb6fac641b66060c4b791452c738cf18a3a9705c3ace123fdd9f90f'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jIK8j9DBSuks"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:29.760488Z",
          "iopub.execute_input": "2021-09-10T09:27:29.760794Z",
          "iopub.status.idle": "2021-09-10T09:27:29.777447Z",
          "shell.execute_reply.started": "2021-09-10T09:27:29.760736Z",
          "shell.execute_reply": "2021-09-10T09:27:29.776349Z"
        },
        "trusted": true,
        "id": "kXCZ-V1JSukw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "html_tables = {}\n",
        "\n",
        "# For every table in the datasets folder...\n",
        "for table_name in os.listdir('../input/sentiments-dataset'):\n",
        "    #this is the path to the file. Don't touch!\n",
        "    table_path = f'../input/sentiments-dataset/{table_name}'\n",
        "    table_file = open(table_path, 'r')\n",
        "    # Read the contents of the file into 'html'\n",
        "    html = BeautifulSoup(table_file)\n",
        "    # Find 'news-table' in the Soup and load it into 'html_table'\n",
        "    html_table = html.find(id=\"news-table\")\n",
        "    # Add the table to our dictionary\n",
        "    html_tables[table_name] = html_table"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:29.779029Z",
          "iopub.execute_input": "2021-09-10T09:27:29.779368Z",
          "iopub.status.idle": "2021-09-10T09:27:30.524305Z",
          "shell.execute_reply.started": "2021-09-10T09:27:29.779332Z",
          "shell.execute_reply": "2021-09-10T09:27:30.523778Z"
        },
        "trusted": true,
        "id": "DhJtZNe0Sukx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read one single day of headlines\n",
        "tsla = html_tables['tsla_22sep.html']\n",
        "# Get all the table rows tagged in HTML with <tr> into 'tesla_tr'\n",
        "tsla_tr = tsla.findAll('tr')\n",
        "\n",
        "# For each row...\n",
        "for i, table_row in enumerate(tsla_tr):\n",
        "    # Read the text of the element 'a' into 'link_text'\n",
        "    link_text = table_row.a.get_text()\n",
        "    # Read the text of the element 'td' into 'data_text'\n",
        "    data_text = table_row.td.get_text()\n",
        "    # Print the count\n",
        "    print(f'{i}:')\n",
        "    # Print the contents of 'link_text' and 'data_text'\n",
        "    print(link_text)\n",
        "    print(data_text)\n",
        "    # The following exits the loop after three rows to prevent spamming the notebook, do not touch\n",
        "    if i == 3:\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:30.526335Z",
          "iopub.execute_input": "2021-09-10T09:27:30.526607Z",
          "iopub.status.idle": "2021-09-10T09:27:30.541731Z",
          "shell.execute_reply.started": "2021-09-10T09:27:30.526561Z",
          "shell.execute_reply": "2021-09-10T09:27:30.536433Z"
        },
        "trusted": true,
        "id": "dX238QjwSukx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hold the parsed news into a list\n",
        "parsed_news = []\n",
        "# Iterate through the news\n",
        "for file_name, news_table in html_tables.items():\n",
        "    # Iterate through all tr tags in 'news_table'\n",
        "    for x in news_table.findAll('tr'):\n",
        "        # Read the text from the tr tag into text\n",
        "        text = x.get_text()\n",
        "        headline = x.a.get_text()\n",
        "        # Split the text in the td tag into a list\n",
        "        date_scrape = x.td.text.split()\n",
        "        # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
        "        # If not, load 'date' as the 1st element and 'time' as the second\n",
        "        if len(date_scrape) == 1:\n",
        "            time = date_scrape[0]\n",
        "        else:\n",
        "            date = date_scrape[0]\n",
        "            time = date_scrape[1]\n",
        "\n",
        "        # Extract the ticker from the file name, get the string up to the 1st '_'\n",
        "        ticker = file_name.split('_')[0]\n",
        "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
        "        parsed_news.append([ticker, date, time, headline])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:30.54277Z",
          "iopub.execute_input": "2021-09-10T09:27:30.543073Z",
          "iopub.status.idle": "2021-09-10T09:27:30.594092Z",
          "shell.execute_reply.started": "2021-09-10T09:27:30.543032Z",
          "shell.execute_reply": "2021-09-10T09:27:30.593521Z"
        },
        "trusted": true,
        "id": "BEL24WFRSuky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK VADER for sentiment analysis\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# New words and values\n",
        "new_words = {\n",
        "    'crushes': 10,\n",
        "    'beats': 5,\n",
        "    'misses': -5,\n",
        "    'trouble': -10,\n",
        "    'falls': -100,\n",
        "}\n",
        "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "# Update the lexicon\n",
        "vader.lexicon.update(new_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:30.595021Z",
          "iopub.execute_input": "2021-09-10T09:27:30.59536Z",
          "iopub.status.idle": "2021-09-10T09:27:32.43289Z",
          "shell.execute_reply.started": "2021-09-10T09:27:30.595321Z",
          "shell.execute_reply": "2021-09-10T09:27:32.432375Z"
        },
        "trusted": true,
        "id": "JCQ7uwYjSuky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use these column names\n",
        "columns = ['ticker', 'date', 'time', 'headline']\n",
        "# Convert the list of lists into a DataFrame\n",
        "scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
        "# Iterate through the headlines and get the polarity scores\n",
        "scores = [vader.polarity_scores(headline) for headline in scored_news.headline.values]\n",
        "# Convert the list of dicts into a DataFrame\n",
        "scores_df = pd.DataFrame(scores)\n",
        "# Join the DataFrames\n",
        "scored_news = pd.concat([scored_news, scores_df], axis=1)\n",
        "# Convert the date column from string to datetime\n",
        "scored_news['date'] = pd.to_datetime(scored_news.date).dt.date"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:32.434578Z",
          "iopub.execute_input": "2021-09-10T09:27:32.434833Z",
          "iopub.status.idle": "2021-09-10T09:27:32.711363Z",
          "shell.execute_reply.started": "2021-09-10T09:27:32.434802Z",
          "shell.execute_reply": "2021-09-10T09:27:32.710262Z"
        },
        "trusted": true,
        "id": "ipWnkwAbSuky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Group by date and ticker columns from scored_news and calculate the mean\n",
        "mean_c = scored_news.groupby(['date', 'ticker']).mean()\n",
        "# Unstack the column ticker\n",
        "mean_c = mean_c.unstack(level=1)\n",
        "# Get the cross-section of compound in the 'columns' axis\n",
        "mean_c = mean_c.xs('compound', axis=1)\n",
        "# Plot a bar chart with pandas\n",
        "mean_c.plot.bar();"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:32.712749Z",
          "iopub.execute_input": "2021-09-10T09:27:32.712982Z",
          "iopub.status.idle": "2021-09-10T09:27:33.1668Z",
          "shell.execute_reply.started": "2021-09-10T09:27:32.712949Z",
          "shell.execute_reply": "2021-09-10T09:27:33.166262Z"
        },
        "trusted": true,
        "id": "XYYtu19gSukz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of headlines in scored_news (store as integer)\n",
        "num_news_before = scored_news.headline.count()\n",
        "# Drop duplicates based on ticker and headline\n",
        "scored_news_clean = scored_news.drop_duplicates(['ticker', 'headline'])\n",
        "# Count number of headlines after dropping duplicates\n",
        "num_news_after = scored_news_clean.headline.count()\n",
        "# Print before and after numbers to get an idea of how we did\n",
        "f\"Before we had {num_news_before} headlines, now we have {num_news_after}\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:33.168149Z",
          "iopub.execute_input": "2021-09-10T09:27:33.168443Z",
          "iopub.status.idle": "2021-09-10T09:27:33.182029Z",
          "shell.execute_reply.started": "2021-09-10T09:27:33.168389Z",
          "shell.execute_reply": "2021-09-10T09:27:33.1807Z"
        },
        "trusted": true,
        "id": "ziX_id7WSuk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the index to ticker and date\n",
        "single_day = scored_news_clean.set_index(['ticker', 'date'])\n",
        "#print(single_day)\n",
        "# Cross-section the fb row\n",
        "single_day = single_day.loc['fb']\n",
        "#print(single_day)\n",
        "# Select the 3rd of January of 2019\n",
        "single_day = single_day.loc['2019-01-03']\n",
        "#print(single_day)\n",
        "# Convert the datetime string to just the time\n",
        "single_day['time'] = pd.to_datetime(single_day['time'])\n",
        "single_day['time'] = single_day.time.dt.time\n",
        "#print(single_day)\n",
        "#print(single_day.shape)\n",
        "# Set the index to time and sort by it\n",
        "single_day.set_index('time', inplace=True)\n",
        "single_day=single_day.sort_index(ascending=True)\n",
        "single_day.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:33.183107Z",
          "iopub.execute_input": "2021-09-10T09:27:33.183345Z",
          "iopub.status.idle": "2021-09-10T09:27:33.220912Z",
          "shell.execute_reply.started": "2021-09-10T09:27:33.183299Z",
          "shell.execute_reply": "2021-09-10T09:27:33.220071Z"
        },
        "trusted": true,
        "id": "P5WF5r8rSuk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TITLE = \"Positive, negative and neutral sentiment for FB on 2019-01-03\"\n",
        "COLORS = [\"red\", \"orange\", \"green\"]\n",
        "# Drop the columns that aren't useful for the plot\n",
        "plot_day = single_day.drop(['headline', 'compound'], axis=1)\n",
        "# Change the column names to 'negative', 'positive', and 'neutral'\n",
        "plot_day.columns = ['negative', 'positive', 'neutral']\n",
        "# Plot a stacked bar chart\n",
        "plot_day.plot.bar(stacked = True,\n",
        "                  figsize=(10, 6),\n",
        "                  title = TITLE,\n",
        "                  color = COLORS)\n",
        "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
        "plt.ylabel(\"scores\");"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-10T09:27:33.222304Z",
          "iopub.execute_input": "2021-09-10T09:27:33.222578Z",
          "iopub.status.idle": "2021-09-10T09:27:33.6742Z",
          "shell.execute_reply.started": "2021-09-10T09:27:33.222493Z",
          "shell.execute_reply": "2021-09-10T09:27:33.673286Z"
        },
        "trusted": true,
        "id": "VwoinMaFSuk1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}